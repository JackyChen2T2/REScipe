{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REScipe.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNJXeUQJ9kj9R2Foo7A5sh8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JackyChen2T2/REScipe/blob/main/REScipe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh4BtR0qeAuC"
      },
      "source": [
        "# for loading google drive files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# for data cleaning\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# for tokenizing english words and applying GloVe representation\n",
        "import spacy\n",
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCd_B5JWlgoO"
      },
      "source": [
        "# flags\n",
        "XLSX_PATH = '/content/drive/My Drive/Colab Notebooks/Project_REScipe/data/total_data.xlsx'      # copy the path of the xlsx file in Path in Google Colab menu\n",
        "XLSX_COLUMN = ['rand_int_0', 'name', 'url', 'rand_int_1', 'size', 'ingredient', 'recipe', 'rand_int_2']\n",
        "ALL_DATA = False"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPFogLA7jM4g"
      },
      "source": [
        "===================== A =====================\n",
        "\n",
        "This section loads the excel file(.xlsx) into a Pandas Dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-92lgp7zidU-",
        "outputId": "1907478c-828e-45f2-a9e7-0c29cf6fa7f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# load the xlsx file from Google Drive to a Pandas Dataframe\n",
        "xlsx_total_data = pd.read_excel(XLSX_PATH, header=None, names=XLSX_COLUMN)\n",
        "\n",
        "print(xlsx_total_data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       rand_int_0  ... rand_int_2\n",
            "0               0  ...     190001\n",
            "1               1  ...     190004\n",
            "2               2  ...     190015\n",
            "3               3  ...     190032\n",
            "4               4  ...     190060\n",
            "...           ...  ...        ...\n",
            "40333           7  ...     149842\n",
            "40334           8  ...     149844\n",
            "40335           9  ...     149846\n",
            "40336          10  ...     149855\n",
            "40337          11  ...     149896\n",
            "\n",
            "[40338 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJPNddBuu_bU"
      },
      "source": [
        "===================== B =====================\n",
        "\n",
        "This section saves recipe names (index 'name') from the Pandas Dataframe to a csv file in the same Google Drive folder.\n",
        "\n",
        "This file (total_name.csv) is a temporary dictionary for debugging topic modeling and LDA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPnS2-3Wrv6r",
        "outputId": "40dacb9d-1be7-46f3-ad7a-bbc34fa35bfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# extract the column of 'name' from the Pandas Dataframe\n",
        "csv_name = xlsx_total_data[['name']]\n",
        "# add an additional index column, in case that tokenization or GloVe representation removes certain recipes\n",
        "csv_name['index'] = csv_name.index\n",
        "\n",
        "csv_path = '/'.join(XLSX_PATH.split(sep='/')[:-1]) + '/total_name.csv'\n",
        "csv_name.to_csv(csv_path, index=False)\n",
        "\n",
        "print(csv_name)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                                             name  index\n",
            "0                             Winter Endive Salad      0\n",
            "1                        Stuffed Artichoke Hearts      1\n",
            "2                      Slow Cooker Moscow Chicken      2\n",
            "3      Slow Cooker Creole Black Beans and Sausage      3\n",
            "4              Champagne Sorbet with Berry Medley      4\n",
            "...                                           ...    ...\n",
            "40333                           Hawaiian Iced Tea  40333\n",
            "40334                    Belgian Endive au Gratin  40334\n",
            "40335                             Hot Cider Punch  40335\n",
            "40336    Hash Brown Casserole for the Slow Cooker  40336\n",
            "40337                       Samhain Pumpkin Bread  40337\n",
            "\n",
            "[40338 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZYWcatDhIb5"
      },
      "source": [
        "===================== C =====================\n",
        "\n",
        "This section loads total_name.csv and saves a tokenized version of this csv back.\n",
        "\n",
        "This file (token_name.csv) is used for topic modeling with LDA, and GloVe representation (optional)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNXDrsss9gZX",
        "outputId": "3407c817-8daf-43eb-d4e0-b0e9307948d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "csv_path = '/'.join(XLSX_PATH.split(sep='/')[:-1]) + '/total_name.csv'\n",
        "csv_name = pd.read_csv(csv_path)\n",
        "\n",
        "# load spaCy english nlp model\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "# tokenize, remove non-ascii chars, lemmatize, remove stop words (safe actions)\n",
        "list_token_name = []\n",
        "for i, row in csv_name.iterrows():\n",
        "  token_name = []\n",
        "  # tokenize and lemmatize\n",
        "  for word in nlp(row['name']):\n",
        "    # remove non-ascii chars\n",
        "    if all(ord(char) < 128 for char in word.lemma_):\n",
        "      text = word.text\n",
        "      lemma = word.lemma_.lower()\n",
        "    else:\n",
        "      text = word.text.encode('ascii','ignore').decode()\n",
        "      lemma = word.lemma_.lower().encode('ascii','ignore').decode()\n",
        "    # remove stop words\n",
        "    if (len(text) > 2) and (len(lemma) > 2) and (word.is_stop == False):\n",
        "        token_name.append(lemma)\n",
        "  if token_name != []:\n",
        "    list_token_name.append([token_name, row['index']])\n",
        "  else:\n",
        "    print(row['index'], '\\t removed by tokenization.')\n",
        "  if i % 1000 == 0:\n",
        "    print('Tokenizing:', i, '~', i+1000)\n",
        "\n",
        "csv_token_name = pd.DataFrame(list_token_name, columns=['token_name', 'index'])\n",
        "print(csv_token_name)\n",
        "\n",
        "csv_path = '/'.join(XLSX_PATH.split(sep='/')[:-1]) + '/token_name.csv'\n",
        "csv_token_name.to_csv(csv_path, index=False)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizing: 0 ~ 1000\n",
            "Tokenizing: 1000 ~ 2000\n",
            "Tokenizing: 2000 ~ 3000\n",
            "Tokenizing: 3000 ~ 4000\n",
            "Tokenizing: 4000 ~ 5000\n",
            "Tokenizing: 5000 ~ 6000\n",
            "Tokenizing: 6000 ~ 7000\n",
            "Tokenizing: 7000 ~ 8000\n",
            "Tokenizing: 8000 ~ 9000\n",
            "Tokenizing: 9000 ~ 10000\n",
            "9359 \t removed by tokenization.\n",
            "9361 \t removed by tokenization.\n",
            "Tokenizing: 10000 ~ 11000\n",
            "Tokenizing: 11000 ~ 12000\n",
            "Tokenizing: 12000 ~ 13000\n",
            "Tokenizing: 13000 ~ 14000\n",
            "Tokenizing: 14000 ~ 15000\n",
            "Tokenizing: 15000 ~ 16000\n",
            "Tokenizing: 16000 ~ 17000\n",
            "Tokenizing: 17000 ~ 18000\n",
            "Tokenizing: 18000 ~ 19000\n",
            "Tokenizing: 19000 ~ 20000\n",
            "Tokenizing: 20000 ~ 21000\n",
            "Tokenizing: 21000 ~ 22000\n",
            "Tokenizing: 22000 ~ 23000\n",
            "Tokenizing: 23000 ~ 24000\n",
            "Tokenizing: 24000 ~ 25000\n",
            "Tokenizing: 25000 ~ 26000\n",
            "Tokenizing: 26000 ~ 27000\n",
            "Tokenizing: 27000 ~ 28000\n",
            "Tokenizing: 28000 ~ 29000\n",
            "Tokenizing: 29000 ~ 30000\n",
            "Tokenizing: 30000 ~ 31000\n",
            "Tokenizing: 31000 ~ 32000\n",
            "Tokenizing: 32000 ~ 33000\n",
            "Tokenizing: 33000 ~ 34000\n",
            "Tokenizing: 34000 ~ 35000\n",
            "Tokenizing: 35000 ~ 36000\n",
            "Tokenizing: 36000 ~ 37000\n",
            "Tokenizing: 37000 ~ 38000\n",
            "Tokenizing: 38000 ~ 39000\n",
            "Tokenizing: 39000 ~ 40000\n",
            "Tokenizing: 40000 ~ 41000\n",
            "                                          token_name  index\n",
            "0                            [winter, endive, salad]      0\n",
            "1                          [stuff, artichoke, heart]      1\n",
            "2                    [slow, cooker, moscow, chicken]      2\n",
            "3      [slow, cooker, creole, black, beans, sausage]      3\n",
            "4                 [champagne, sorbet, berry, medley]      4\n",
            "...                                              ...    ...\n",
            "40331                          [hawaiian, iced, tea]  40333\n",
            "40332                      [belgian, endive, gratin]  40334\n",
            "40333                            [hot, cider, punch]  40335\n",
            "40334         [hash, brown, casserole, slow, cooker]  40336\n",
            "40335                      [samhain, pumpkin, bread]  40337\n",
            "\n",
            "[40336 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRsI_DneypNk"
      },
      "source": [
        "# remove overly frequent/infrequent words (risky actions, require visualization)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}